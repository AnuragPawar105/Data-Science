{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2baad50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy                                                  \n",
    "from spacy.lang.en import English                             \n",
    "nlp = spacy.load(\"en_core_web_sm\")                            \n",
    "f1 = open('scifiscripts_intro.txt')\n",
    "contents1 = f1.read()                                             \n",
    "text1 = str(contents1)\n",
    "f2 = open('Raw_data_for_analysis.txt')\n",
    "contents2 = f2.read()                                             \n",
    "text2 = str(contents2)\n",
    "f3 = open('Random.txt')\n",
    "contents3 = f3.read()                                             \n",
    "text3 = str(contents3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1afbd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words in spacy is 326\n",
      "First 15 stop words are ['becoming', 'even', 'call', 'two', 'amount', 'on', '‘m', 'himself', '‘re', 'alone', 'will', 'ourselves', 'mine', 'not', 'am']\n"
     ]
    }
   ],
   "source": [
    "# TASK 1\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS                   \n",
    "print('Number of stop words in spacy is %d' % len(spacy_stopwords))           \n",
    "print('First 15 stop words are %s' % list(spacy_stopwords)[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e33d71a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Lemmatization:\n",
      "cries ->  cry\n",
      "this ->  this\n",
      "lied ->  lie\n",
      "computing ->  computing\n",
      "organizing ->  organizing\n",
      "matches ->  match\n",
      "\n",
      " With Stemming:\n",
      "cries  -->  cri\n",
      "this  -->  this\n",
      "lied  -->  lie\n",
      "computing  -->  comput\n",
      "organizing  -->  organ\n",
      "matches  -->  match\n"
     ]
    }
   ],
   "source": [
    "# TASK 2\n",
    "import nltk                                                                                 \n",
    "from nltk.stem.snowball import SnowballStemmer                                             \n",
    "stemmer = SnowballStemmer(language='english')\n",
    "tokens=['cries', 'this', 'lied', 'computing', 'organizing', 'matches']\n",
    "Tokens='cries this lied computing organizing matches'\n",
    "token=nlp(Tokens)\n",
    "print('With Lemmatization:')\n",
    "for word in token:                                                   \n",
    "    print(word.text,'-> ',word.lemma_)\n",
    "print('\\n With Stemming:')\n",
    "for word in tokens:\n",
    "    print(word, ' --> ', stemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ae9d1",
   "metadata": {},
   "source": [
    "### As we can see here Lemmatization gives a better result than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00cebe0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removal of stop word :\n",
      " [Note, poster, Kubrick, newsgroup, :, \n",
      "\n",
      ", found, bbs, ago, thought, pass, \n",
      ", Kubrick, freaks, ., \n",
      "\n",
      ", 02/23/89, \n",
      ", Transcriber, note, :, \n",
      "\n",
      ", Clarke, /, Kubrick/2001, fans, ,, \n",
      "\n",
      ", found, original, paper, copy, screenplay, felt, \n",
      ", compelled, transcribe, disk, upload, bulletin, \n",
      ", boards, enjoyment, ., \n",
      "\n",
      ", final, movie, deviates, screenplay, number, interesting, \n",
      ", ways, ., tried, maintain, format, original, document, \n",
      ", number, lines, page, original, ., order, reduce, \n",
      ", length, file, bar, \", ------, \", delimit, pages, \n",
      ", lot, whitespace, original, screenplay, page, .]\n"
     ]
    }
   ],
   "source": [
    "# TASK 3 a\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "doc1 = nlp(text1)  \n",
    "doc_new= []\n",
    "for word in doc1:                                                      \n",
    "    if word.is_stop==False:                                               \n",
    "        doc_new.append(word)                                    \n",
    "print(\"After removal of stop word :\\n\",doc_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5396482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removal of stop word from a1:\n",
      " [Modi, prime, minister, India, .]\n",
      "After removal of stop word from b1:\n",
      " [Sun, rises, east, .]\n",
      "After removal of stop word from c1:\n",
      " [Peacock, national, bird, India, .]\n",
      "After removal of stop word from d1:\n",
      " [Tiger, national, animal, India, .]\n",
      "After removal of stop word from e1:\n",
      " [Sun, sets, west]\n"
     ]
    }
   ],
   "source": [
    "# TASK 3b\n",
    "a=\"Modi is the prime minister of India.\"\n",
    "b=\"Sun rises in the east.\"\n",
    "c=\"Peacock is the national bird of India.\"\n",
    "d=\"Tiger is the national animal of India.\"\n",
    "e=\"Sun sets in the west\"\n",
    "a1=nlp(a)\n",
    "b1=nlp(b)\n",
    "c1=nlp(c)\n",
    "d1=nlp(d)\n",
    "e1=nlp(e)\n",
    "a2=[]\n",
    "b2=[]\n",
    "c2=[]\n",
    "d2=[]\n",
    "e2=[]\n",
    "for word in a1:                                                      \n",
    "    if word.is_stop==False:                                               \n",
    "        a2.append(word)                                    \n",
    "print(\"After removal of stop word from a1:\\n\",a2)\n",
    "for word in b1:                                                      \n",
    "    if word.is_stop==False:                                               \n",
    "        b2.append(word)                                    \n",
    "print(\"After removal of stop word from b1:\\n\",b2)\n",
    "for word in c1:                                                      \n",
    "    if word.is_stop==False:                                               \n",
    "        c2.append(word)                                    \n",
    "print(\"After removal of stop word from c1:\\n\",c2)\n",
    "for word in d1:                                                      \n",
    "    if word.is_stop==False:                                               \n",
    "        d2.append(word)                                    \n",
    "print(\"After removal of stop word from d1:\\n\",d2)\n",
    "for word in e1:                                                      \n",
    "    if word.is_stop==False:                                               \n",
    "        e2.append(word)                                    \n",
    "print(\"After removal of stop word from e1:\\n\",e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fd32964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bryan \t\t PROPN \t NNP\n",
      "visited \t\t VERB \t VBD\n",
      "his \t\t PRON \t PRP$\n",
      "frined \t\t VERB \t VBN\n",
      "for \t\t ADP \t IN\n",
      "a \t\t DET \t DT\n",
      "while \t\t NOUN \t NN\n",
      "and \t\t CCONJ \t CC\n",
      "then \t\t ADV \t RB\n",
      "went \t\t VERB \t VBD\n",
      "home \t\t ADV \t RB\n",
      "at \t\t ADP \t IN\n",
      "10 \t\t NUM \t CD\n",
      "pm \t\t NOUN \t NN\n",
      ". \t\t PUNCT \t .\n"
     ]
    }
   ],
   "source": [
    "# TASK 4\n",
    "sen=\"Bryan visited his frined for a while and then went home at 10 pm.\"\n",
    "sen1=nlp(sen)\n",
    "for word in sen1:                                       \n",
    "    print(word.text,'\\t\\t',word.pos_,'\\t',word.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53ed77a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proper nouns are: \n",
      " [PADUA, HIGH, SCHOOL, DAY, Revision, November]\n",
      "\n",
      "Numbers are: \n",
      " [12, 1997]\n"
     ]
    }
   ],
   "source": [
    "# TASK 5\n",
    "doc3=nlp(text3)\n",
    "proper_noun=[]\n",
    "number=[]\n",
    "for word in doc3:\n",
    "    if word.pos_== 'PROPN':                                               \n",
    "        proper_noun.append(word)\n",
    "print('Proper nouns are: \\n',proper_noun)\n",
    "for word in doc3:\n",
    "    if word.pos_== 'NUM':                                               \n",
    "        number.append(word)\n",
    "print('\\nNumbers are: \\n',number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae09ba2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'becoming', 'even', 'call', 'two', 'amount', 'on', '‘m', 'himself', '‘re', 'alone', 'will', 'ourselves', 'mine', 'not', 'am', 'must', 'latterly', 'some', '’s', 'herself', 'her', 'therein', 'behind', 'it', 'whence', 'might', 'eleven', 'fifty', 'whereafter', 'throughout', 'ca', 'again', 'beside', 'wherein', 'back', 'out', 'to', \"n't\", 'then', 'done', 'yourselves', 'anyone', 'hundred', 'please', 'one', \"'ve\", 'me', 'via', 'once', 'an', 'whether', 'made', 'who', 'whose', 'bottle', '’m', 'every', 'four', 'tree', 'therefore', 'always', 'upon', 'everywhere', 'anyway', 'they', 'thru', 'within', 'did', \"'d\", 'doing', 'only', 'more', 'regarding', 'does', 'herein', 'afterwards', 'being', 'bottom', 'at', 'but', \"'m\", 'before', 'nor', 'least', 'be', 'together', 'yet', 'also', 'never', \"'re\", 'across', 'show', 'seemed', 'his', 'because', 'something', 'say', 'and', 'hence', 'just', 'thereafter', 'third', 'chair', 'namely', 'too', 'few', 'else', 'nine', 'although', 'ever', 'after', 'however', 'towards', 'seems', 'cannot', 'onto', '‘s', 'seem', 'twelve', 'he', 'when', 'which', 'those', '’d', 'several', 'somehow', 'side', \"'s\", 'move', 'nothing', 'part', 'them', 'various', 'six', 'further', 'may', 'latter', 'myself', 'last', 'except', 'this', 'could', 'name', 'from', 'what', 'we', 'seeming', 'same', 'about', 'much', 'such', 'though', 'top', 'hereupon', 'how', 'among', 'whom', 'get', 'empty', 'him', 'own', 'can', 'take', 'sometimes', 'i', 'indeed', 'next', 'their', 'anyhow', 'you', 'elsewhere', 'yourself', 'during', 'most', 'formerly', 'now', 'each', 'yours', 'she', 'without', 'due', 'mostly', 'enough', 'whereupon', 'than', 'in', 're', 'our', 'whatever', '’ve', 'fifteen', 'whither', 'so', 'nevertheless', 'had', 'my', 'along', 'the', 'quite', 'ours', 'none', 'is', 'everything', 'all', 'thus', 'used', 'put', 'a', '‘ve', 'have', 'itself', 'five', 'been', 'eight', 'if', 'serious', 'has', 'no', 'by', 'until', 'should', 'nobody', 'over', 'jar', 'whereby', 'while', 'using', 'often', 'everyone', 'hereby', 'hers', 'up', 'thereupon', 'very', 'became', '‘ll', 'into', 'why', 'toward', 'make', 'through', 'its', 'ten', 'hereafter', 'down', 'give', 'otherwise', 'that', 'well', 'others', 'sixty', 'somewhere', 'almost', 'already', 'any', 'whoever', 'do', 'under', \"'ll\", 'per', 'whenever', 'anywhere', 'neither', 'unless', 'another', 'above', 'many', 'for', 'with', 'would', 'forty', 'both', 'whole', 'meanwhile', 'beforehand', 'moreover', 'see', 'below', 'since', 'amongst', 'there', 'photo', 'twenty', 'rather', 'front', 'us', 'of', 'was', 'three', 'thence', 'wherever', 'full', 'around', 'nowhere', '’re', 'are', 'n’t', 'former', 'as', 'against', 'really', 'still', 'off', 'n‘t', 'becomes', 'perhaps', 'were', '‘d', 'either', 'besides', 'where', 'go', '’ll', 'whereas', 'themselves', 'between', 'become', 'beyond', 'anything', 'your', 'or', 'here', 'other', 'less', 'someone', 'keep', 'thereby', 'first', 'sometime', 'noone', 'these'}\n"
     ]
    }
   ],
   "source": [
    "# TASK 6\n",
    "nlp.Defaults.stop_words |= {\"bottle\",\"jar\",\"tree\",\"photo\",\"chair\"}   \n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6c5ee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'becoming', 'even', 'call', 'two', 'amount', 'on', '‘m', 'himself', '‘re', 'alone', 'will', 'ourselves', 'mine', 'not', 'am', 'must', 'latterly', 'some', '’s', 'herself', 'her', 'therein', 'behind', 'it', 'whence', 'might', 'eleven', 'fifty', 'whereafter', 'throughout', 'ca', 'again', 'beside', 'wherein', 'back', 'out', 'to', \"n't\", 'then', 'done', 'yourselves', 'anyone', 'hundred', 'please', 'one', \"'ve\", 'me', 'via', 'once', 'an', 'whether', 'made', 'who', 'whose', 'bottle', '’m', 'every', 'four', 'tree', 'therefore', 'upon', 'everywhere', 'anyway', 'they', 'thru', 'within', 'did', \"'d\", 'doing', 'only', 'more', 'regarding', 'does', 'herein', 'afterwards', 'being', 'bottom', 'at', 'but', \"'m\", 'before', 'nor', 'least', 'be', 'together', 'yet', 'also', \"'re\", 'across', 'show', 'seemed', 'his', 'because', 'something', 'say', 'and', 'hence', 'just', 'thereafter', 'third', 'chair', 'namely', 'too', 'few', 'else', 'nine', 'although', 'ever', 'after', 'however', 'towards', 'seems', 'cannot', 'onto', '‘s', 'seem', 'twelve', 'he', 'when', 'which', 'those', '’d', 'several', 'somehow', 'side', \"'s\", 'move', 'nothing', 'part', 'them', 'various', 'six', 'further', 'may', 'latter', 'myself', 'last', 'except', 'this', 'could', 'name', 'from', 'what', 'we', 'seeming', 'same', 'about', 'much', 'such', 'though', 'top', 'hereupon', 'how', 'among', 'whom', 'get', 'empty', 'him', 'own', 'can', 'take', 'sometimes', 'i', 'indeed', 'next', 'their', 'anyhow', 'you', 'elsewhere', 'yourself', 'during', 'most', 'formerly', 'now', 'each', 'yours', 'she', 'without', 'due', 'mostly', 'enough', 'whereupon', 'than', 'in', 're', 'our', 'whatever', '’ve', 'fifteen', 'whither', 'so', 'nevertheless', 'had', 'my', 'along', 'the', 'quite', 'ours', 'none', 'is', 'everything', 'all', 'thus', 'used', 'put', 'a', '‘ve', 'have', 'itself', 'five', 'been', 'eight', 'if', 'serious', 'has', 'no', 'by', 'until', 'should', 'nobody', 'over', 'jar', 'whereby', 'while', 'using', 'often', 'everyone', 'hereby', 'hers', 'up', 'thereupon', 'very', 'became', '‘ll', 'into', 'why', 'toward', 'make', 'through', 'its', 'ten', 'hereafter', 'down', 'give', 'otherwise', 'that', 'well', 'others', 'sixty', 'somewhere', 'almost', 'already', 'any', 'whoever', 'do', 'under', \"'ll\", 'per', 'whenever', 'anywhere', 'neither', 'unless', 'another', 'above', 'many', 'for', 'with', 'would', 'forty', 'both', 'whole', 'meanwhile', 'beforehand', 'moreover', 'see', 'below', 'since', 'amongst', 'there', 'photo', 'twenty', 'rather', 'front', 'us', 'of', 'was', 'three', 'thence', 'wherever', 'full', 'around', 'nowhere', '’re', 'are', 'n’t', 'former', 'as', 'against', 'really', 'still', 'off', 'n‘t', 'perhaps', 'were', '‘d', 'either', 'besides', 'where', 'go', '’ll', 'whereas', 'themselves', 'become', 'beyond', 'anything', 'your', 'or', 'here', 'other', 'less', 'someone', 'keep', 'thereby', 'first', 'sometime', 'noone', 'these'}\n"
     ]
    }
   ],
   "source": [
    "nlp.Defaults.stop_words -= {\"always\", \"never\",\"between\",\"becomes\"} \n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9ea6d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['note', 'poster', 'Kubrick', 'newsgroup', ':', '\\n\\n', 'find', 'bbs', 'ago', 'think', 'pass', '\\n', 'Kubrick', 'freak', '.', '\\n\\n', '02/23/89', '\\n', 'Transcriber', 'note', ':', '\\n\\n', 'Clarke', '/', 'Kubrick/2001', 'fan', ',', '\\n\\n', 'find', 'original', 'paper', 'copy', 'screenplay', 'feel', '\\n', 'compel', 'transcribe', 'disk', 'upload', 'bulletin', '\\n', 'board', 'enjoyment', '.', '\\n\\n', 'final', 'movie', 'deviate', 'screenplay', 'number', 'interesting', '\\n', 'way', '.', 'try', 'maintain', 'format', 'original', 'document', '\\n', 'number', 'line', 'page', 'original', '.', 'order', 'reduce', '\\n', 'length', 'file', 'bar', '\"', '------', '\"', 'delimit', 'page', '\\n', 'lot', 'whitespace', 'original', 'screenplay', 'page', '.'] \n",
      "\n",
      "Note \t\t VERB \t VB\n",
      "from \t\t ADP \t IN\n",
      "poster \t\t NOUN \t NN\n",
      "to \t\t ADP \t IN\n",
      "Kubrick \t\t PROPN \t NNP\n",
      "newsgroup \t\t NOUN \t NN\n",
      ": \t\t PUNCT \t :\n",
      "\n",
      "\n",
      " \t\t SPACE \t _SP\n",
      "I \t\t PRON \t PRP\n",
      "found \t\t VERB \t VBD\n",
      "this \t\t PRON \t DT\n",
      "on \t\t ADP \t IN\n",
      "a \t\t DET \t DT\n",
      "bbs \t\t NOUN \t NN\n",
      "a \t\t DET \t DT\n",
      "while \t\t NOUN \t NN\n",
      "ago \t\t ADV \t RB\n",
      "and \t\t CCONJ \t CC\n",
      "I \t\t PRON \t PRP\n",
      "thought \t\t VERB \t VBD\n",
      "I \t\t PRON \t PRP\n",
      "'d \t\t AUX \t MD\n",
      "pass \t\t VERB \t VB\n",
      "it \t\t PRON \t PRP\n",
      "along \t\t ADP \t RP\n",
      "to \t\t ADP \t IN\n",
      "all \t\t DET \t DT\n",
      "\n",
      " \t\t SPACE \t _SP\n",
      "of \t\t ADP \t IN\n",
      "you \t\t PRON \t PRP\n",
      "Kubrick \t\t PROPN \t NNP\n",
      "freaks \t\t VERB \t VBZ\n",
      "out \t\t ADP \t RP\n",
      "there \t\t ADV \t RB\n",
      ". \t\t PUNCT \t .\n",
      "\n",
      "\n",
      " \t\t SPACE \t _SP\n",
      "02/23/89 \t\t PROPN \t NNP\n",
      "\n",
      " \t\t SPACE \t _SP\n",
      "Transcriber \t\t PROPN \t NNP\n",
      "'s \t\t PART \t POS\n",
      "note \t\t NOUN \t NN\n",
      ": \t\t PUNCT \t :\n",
      "\n",
      "\n",
      " \t\t SPACE \t _SP\n",
      "For \t\t ADP \t IN\n",
      "all \t\t DET \t DT\n",
      "you \t\t PRON \t PRP\n",
      "Clarke \t\t PROPN \t NNP\n",
      "/ \t\t SYM \t SYM\n",
      "Kubrick/2001 \t\t PROPN \t NNP\n",
      "fans \t\t NOUN \t NNS\n",
      ", \t\t PUNCT \t ,\n",
      "\n",
      "\n",
      " \t\t SPACE \t _SP\n",
      "I \t\t PRON \t PRP\n",
      "found \t\t VERB \t VBD\n",
      "the \t\t DET \t DT\n",
      "original \t\t ADJ \t JJ\n",
      "paper \t\t NOUN \t NN\n",
      "copy \t\t NOUN \t NN\n",
      "of \t\t ADP \t IN\n",
      "this \t\t DET \t DT\n",
      "screenplay \t\t NOUN \t NN\n",
      "a \t\t DET \t DT\n",
      "while \t\t NOUN \t NN\n",
      "back \t\t ADV \t RB\n",
      "and \t\t CCONJ \t CC\n",
      "felt \t\t VERB \t VBD\n",
      "\n",
      " \t\t SPACE \t _SP\n",
      "compelled \t\t VERB \t VBN\n",
      "to \t\t PART \t TO\n",
      "transcribe \t\t VERB \t VB\n",
      "it \t\t PRON \t PRP\n",
      "to \t\t PART \t TO\n",
      "disk \t\t VERB \t VB\n",
      "and \t\t CCONJ \t CC\n",
      "upload \t\t VERB \t VB\n",
      "it \t\t PRON \t PRP\n",
      "to \t\t ADP \t IN\n",
      "various \t\t ADJ \t JJ\n",
      "bulletin \t\t NOUN \t NN\n",
      "\n",
      " \t\t SPACE \t _SP\n",
      "boards \t\t NOUN \t NNS\n",
      "for \t\t ADP \t IN\n",
      "the \t\t DET \t DT\n",
      "enjoyment \t\t NOUN \t NN\n",
      "of \t\t ADP \t IN\n",
      "all \t\t PRON \t DT\n",
      ". \t\t PUNCT \t .\n",
      "\n",
      "\n",
      " \t\t SPACE \t _SP\n",
      "The \t\t DET \t DT\n",
      "final \t\t ADJ \t JJ\n",
      "movie \t\t NOUN \t NN\n",
      "deviates \t\t NOUN \t NNS\n",
      "from \t\t ADP \t IN\n",
      "this \t\t DET \t DT\n",
      "screenplay \t\t NOUN \t NN\n",
      "in \t\t ADP \t IN\n",
      "a \t\t DET \t DT\n",
      "number \t\t NOUN \t NN\n",
      "of \t\t ADP \t IN\n",
      "interesting \t\t ADJ \t JJ\n",
      "\n",
      " \t\t SPACE \t _SP\n",
      "ways \t\t NOUN \t NNS\n",
      ". \t\t PUNCT \t .\n",
      "I \t\t PRON \t PRP\n",
      "'ve \t\t AUX \t VBP\n",
      "tried \t\t VERB \t VBN\n",
      "to \t\t PART \t TO\n",
      "maintain \t\t VERB \t VB\n",
      "the \t\t DET \t DT\n",
      "format \t\t NOUN \t NN\n",
      "of \t\t ADP \t IN\n",
      "the \t\t DET \t DT\n",
      "original \t\t ADJ \t JJ\n",
      "document \t\t NOUN \t NN\n",
      "except \t\t SCONJ \t IN\n",
      "\n",
      " \t\t SPACE \t _SP\n",
      "the \t\t DET \t DT\n",
      "number \t\t NOUN \t NN\n",
      "of \t\t ADP \t IN\n",
      "lines \t\t NOUN \t NNS\n",
      "per \t\t ADP \t IN\n",
      "page \t\t NOUN \t NN\n",
      "of \t\t ADP \t IN\n",
      "the \t\t DET \t DT\n",
      "original \t\t ADJ \t JJ\n",
      ". \t\t PUNCT \t .\n",
      "In \t\t ADP \t IN\n",
      "order \t\t NOUN \t NN\n",
      "to \t\t PART \t TO\n",
      "reduce \t\t VERB \t VB\n",
      "the \t\t DET \t DT\n",
      "\n",
      " \t\t SPACE \t _SP\n",
      "length \t\t NOUN \t NN\n",
      "of \t\t ADP \t IN\n",
      "this \t\t DET \t DT\n",
      "file \t\t NOUN \t NN\n",
      "I \t\t PRON \t PRP\n",
      "'ve \t\t AUX \t VBP\n",
      "used \t\t VERB \t VBN\n",
      "a \t\t DET \t DT\n",
      "bar \t\t NOUN \t NN\n",
      "of \t\t ADP \t IN\n",
      "\" \t\t PUNCT \t ``\n",
      "------ \t\t PUNCT \t :\n",
      "\" \t\t PUNCT \t ''\n",
      "to \t\t PART \t TO\n",
      "delimit \t\t VERB \t VB\n",
      "the \t\t DET \t DT\n",
      "pages \t\t NOUN \t NNS\n",
      "as \t\t SCONJ \t IN\n",
      "\n",
      " \t\t SPACE \t _SP\n",
      "there \t\t PRON \t EX\n",
      "was \t\t VERB \t VBD\n",
      "a \t\t DET \t DT\n",
      "lot \t\t NOUN \t NN\n",
      "of \t\t ADP \t IN\n",
      "whitespace \t\t NOUN \t NN\n",
      "per \t\t ADP \t IN\n",
      "original \t\t ADJ \t JJ\n",
      "screenplay \t\t NOUN \t NN\n",
      "page \t\t NOUN \t NN\n",
      ". \t\t PUNCT \t .\n"
     ]
    }
   ],
   "source": [
    "# TASK 7\n",
    "doc2=nlp(text1)\n",
    "new_doc2=[]\n",
    "for word in doc2:                                                      \n",
    "    if word.is_stop==False:                                               \n",
    "        new_doc2.append(word)\n",
    "new_docl=[]\n",
    "for word in new_doc2:\n",
    "    new_docl.append(word.lemma_)\n",
    "print(new_docl,'\\n')\n",
    "for word in doc2:                                      \n",
    "    print(word.text,'\\t\\t',word.pos_,'\\t',word.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c035fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff24b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
